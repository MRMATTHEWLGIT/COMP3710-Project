"""
modules.py created by Matthew Lockett 46988133
"""
import torch
import torch.nn as nn
import torch.nn.functional as F
import hyperparameters as hp

class MappingNetwork(nn.Module):
    """
    Within a StyleGAN model, the Mapping Network is used to transform a latent space vector z
    (a vector of random noise) into an intermediate latent vector w. The intermediate latent space
    vector is used to provide more control over the images features when compared to a normal GAN 
    model.

    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    """
    
    def __init__(self):
        """
        An instance of the MappingNetwork StyleGAN model.
        """
        super(MappingNetwork, self).__init__()

        layers = []
        layers.append(PixelNorm()) # Start by normalising the latent vector

        # For each layer complete a pass of one fully connected neural network and save
        for i in range(hp.MAPPING_LAYERS):
            layers.append(fully_connected(hp.LATENT_SIZE, hp.LATENT_SIZE))

        # Store all fully connected layers sequentially
        self.mapping = nn.Sequential(*layers)


    def forward(self, z):
        """
        Performs a forward pass on an input latent space vector, and turns it into
        an intermediate latent space vector w.
        
        Param: z: A latent space vector
        Return: The intermediate latent space vector w
        """
        return self.mapping(z)
    

class Generator(nn.Module):
    """
    Within the StyleGAN model, a generator is used to produce an output image that should accurately 
    represent the desired image classification, or in this case the ADNI dataset images. This generator 
    incorporates a mapping network which produces a style vector to influence the style of the 
    output image. It also uses a series of generator layers, which perform convolutions, noise injections
    and AdaIN passes to converge the feature map into the desired output image. 

    For the ADNI dataset, the images are of resolution 256x256, and thus this generator will start with a constant 
    4x4 resolution feature map, and scale it up to the full 256x256 resolution required, as a greyscale image.
    
    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    """

    def __init__(self):
        """
        An instance of the Generator class. Each generator requires a mapping network, a series of generator layers
        and a final output of an image with the number of channels being hp.NUM_CHANNELS.
        """
        super(Generator, self).__init__()

        # The first generator layer needs to start with a constant learned input feature map
        self.constant = nn.Parameter(torch.ones((1, hp.LATENT_SIZE, 4, 4)))

        # Initialise the mapping network for this generator 
        self.mapping = MappingNetwork()

        # Models all generator layers from a 4x4 feature map up to a scaled 256x256 feature map
        self.gen_layers = nn.ModuleList([
            GenLayer(hp.LATENT_SIZE, hp.LATENT_SIZE, first_layer=True), # Output: 4x4 Feature Map
            GenLayer(hp.LATENT_SIZE, hp.LATENT_SIZE // 2),              # Output: 8x8 Feature Map
            GenLayer(hp.LATENT_SIZE // 2, hp.LATENT_SIZE // 4),         # Output: 16x16 Feature Map
            GenLayer(hp.LATENT_SIZE // 4, hp.LATENT_SIZE // 8),         # Output: 32x32 Feature Map
            GenLayer(hp.LATENT_SIZE // 8, hp.LATENT_SIZE // 16),        # Output: 64x64 Feature Map
            GenLayer(hp.LATENT_SIZE // 16, hp.LATENT_SIZE // 32),       # Output: 128x128 Feature Map
            GenLayer(hp.LATENT_SIZE // 32, hp.LATENT_SIZE // 64),       # Output: 256x256 Feature Map
        ])

        # Used to convert the feature map back into a greyscale image
        self.final_output = nn.Conv2d(hp.LATENT_SIZE // 64, hp.NUM_CHANNELS, kernel_size=1)


    def forward(self, z1, z2=None, mixing_ratio=hp.MIXING_RATIO):
        """
        Completes a forward pass of the generator, fully converting the 4x4 constant feature map into a full 
        resolution 256x256 greyscale image resembling the ADNI dataset images.

        Param: z1: A latent space vector input into the mapping network. 
        Param: z2: A potential second latent space vector used for mixing regularisation.
        Param: mixing_ratio: A ratio used for mixing regularisation.
        Return: A 256x256 greyscale image, based on the ADNI dataset.
        REF: The mixing regularisation portion of this function was inspired by code generated by ChatGPT-4o
        REF: Based on the following prompt: How can I implement mixing regularization into my StyleGAN model?
        """
        # Generate a style vector
        w1 = self.mapping(z1)

        # Apply mixing of style vectors if required
        if z2 is not None:
            w2 = self.mapping(z2)
            w = self.mix_style_vectors(w1, w2, mixing_ratio)
        else:
            w = w1

        # Create the constant feature map input into the generator
        x = self.constant

        # Pass the feature map and mixed style vector into each layer of the generator
        for layer in self.gen_layers:
            x = layer(x, w)
            
        return torch.tanh(self.final_output(x))
    

    def mix_style_vectors(self, w1, w2, mixing_ratio):
        """
        Given two style vectors, mixes them based on the mixing_ratio.

        Param: w1: A style vector. 
        Param: w2: Another style vector.
        Param: mixing_ratio :A mixing ratio used to combine the two style vectors.
        Returns: A mixed style vector of w1 and w2.
        REF: This function was inspired by code generated by ChatGPT-4o
        REF: Based on the following prompt: How can I implement mixing regularization into my StyleGAN model?
        """
        return w1 * (1 - mixing_ratio) + w2 * mixing_ratio


class GenLayer(nn.Module):
    """
    Within the StyleGAN model, the generator will need multiple layers to upscale the 
    initial feature map size into the desired resolution of the final output image. Each layer
    also consists of a series of convolutions, noise injections and AdaIN passes, that are 
    ultimately used to generate desired feature maps within the generator. 
    
    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    REF: It was also inspired by the following website: 
    REF: https://blog.paperspace.com/implementation-stylegan-from-scratch/ 
    """

    def __init__(self, in_channels, out_channels, first_layer=False):
        """
        An instance of one GenLayer class. Each layer consists of multiple convolutions,
        noise injections and AdaIN passes. 

        Param: in_channels: The number of channels for the input feature map, x. 
        Param: out_channels: The number of channels for the output feature map of this layer
        Param: upscale: Determines if the feature map needs to be upscaled or not.
        """
        super(GenLayer, self).__init__()

        # Determines if this GenLayer is the first layer of the generator
        self.first_layer = first_layer

        # Model both convolutions needed with a 3x3 kernel
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)

        # Model the noise injection into each layer, after each convolution
        self.noise1 = Noise(out_channels)
        self.noise2 = Noise(out_channels)

        # Need to perform a non-linear activation after the noise injection 
        self.lrelu = nn.LeakyReLU(hp.LRELU_SLOPE_ANGLE, inplace=True)

        # Model the Adaptive Instance Normalisation (AdaIN) after each noise injection
        self.adain1 = AdaIN(out_channels)
        self.adain2 = AdaIN(out_channels)


    def forward(self, x, w):
        """
        Performs a forward pass of one generator layer. There is a distinction between the first 
        layer of the generator and subsequent layers, wherein the first layer does not need upscaling
        and an extra convolution is not needed.

        Param: x: The input feature map into this generator layer. 
        Param: w: The input style vector into this generator layer.
        Returns: x: An altered feature map, based on one GenLayer pass.
        """
        # This is the first layer of the generator, do not include upscaling or extra conv
        if self.first_layer:
            x = self.adain1(self.lrelu(self.noise1(x)), w)

        # Upscale by doubling the resolution of the feature map and apply extra conv
        else:
            x = F.interpolate(x, scale_factor=2, mode="bilinear", align_corners=False)
            x = self.adain1(self.lrelu(self.noise1(self.conv1(x))), w)

        x = self.adain2(self.lrelu(self.noise2(self.conv2(x))), w)

        return x
    
    
class Discriminator(nn.Module):
    """
    Within the StyleGAN model, the Discriminator is used to classify an input image as either being 
    real or fake. This is done by taking in an input image and applying a series of DiscLayers
    to the image to reduce it down to a tensor of size [batch_size, 1] which will contain scalar
    values indicating the probability that the input image is fake or not.

    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    REF: It was also inspired by the following website: 
    REF: https://blog.paperspace.com/implementation-stylegan-from-scratch/ 
    """

    def __init__(self):
        """
        An instance of the Discriminator class. A seires of DiscLayer's are initialised which take
        an input image of 256x256 resolution and convert it down to a set of scalar values. These 
        scalar values are then converted to between 0 and 1 by a sigmoid function.
        """
        super(Discriminator, self).__init__()

        self.disc_layers = nn.ModuleList([
            DiscLayer(hp.NUM_CHANNELS, hp.LATENT_SIZE // 64),        # Output: 128x128 Feature Map
            DiscLayer(hp.LATENT_SIZE // 64, hp.LATENT_SIZE // 32),  # Output: 64x64 Feature Map
            DiscLayer(hp.LATENT_SIZE // 32, hp.LATENT_SIZE // 16),  # Output: 32x32 Feature Map
            DiscLayer(hp.LATENT_SIZE // 16, hp.LATENT_SIZE // 8),   # Output: 16x16 Feature Map
            DiscLayer(hp.LATENT_SIZE // 8, hp.LATENT_SIZE // 4),    # Output: 8x8 Feature Map
            DiscLayer(hp.LATENT_SIZE // 4, hp.LATENT_SIZE // 2),    # Output: 4x4 Feature Map
        ])

        self.final_output = nn.Conv2d(hp.LATENT_SIZE // 2, 1, kernel_size=4, stride=1, padding=0)

        self.sigmoid = nn.Sigmoid()


    def forward(self, x):
        """
        Represents a forward pass of the Discriminator, which takes in a given input image 'x'
        and outputs scalar values determining if the input image is real or fake.

        Param: x: An input image of resolution 256x256, from the ADNI dataset.
        Returns: A set of scalar values determining if the input image is real or fake.
        """

        # Input the feature map into each layer of the discriminator
        for layer in self.disc_layers:
            x = layer(x)

        # Performs a final convolution on the feature map to convert it to a channel of 1
        x = self.final_output(x)

        # Flattens the dimensions to [batch_size, 1] for output
        x = torch.flatten(x, start_dim=1)

        # Converts raw scores into a range between 0 and 1
        x = self.sigmoid(x)

        return x
    

class DiscLayer(nn.Module):
    """
    Within the StyleGAN model, the discriminator will need a series of layers to downscale the
    256x256 input image into a single real number. The DiscLayer represents one of these layers, 
    and uses two convolutions for feature extraction and downsamples by average pooling the 
    feature map, effectively reducing the feature map resolution in half with each layer.

    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    """

    def __init__(self, in_channels, out_channels):
        """
        An instance of the DiscLayer class. Each layer has two convolutions for feature extraction and
        an average pool function to downscale the feature map.

        Param: in_channels: The number of channels for the input feature map, x. 
        Param: out_channels: The number of channels for the output feature map of this layer
        """
        super(DiscLayer, self).__init__()

        # Model both convolutions needed with a 3x3 kernel
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

        # Need to perform a non-linear activation after each convolution
        self.lrelu = nn.LeakyReLU(hp.LRELU_SLOPE_ANGLE, inplace=True)

        # Downscale the feature map by a factor of 2
        self.downscale = nn.AvgPool2d(kernel_size=2, stride=2)


    def forward(self, x):
        """
        Represents one froward pass of a discriminator layer. The input feature map x has 
        it's features extracted via two convolutional layers and is then downscaled by a 
        factor of 2.

        Params: x: The input feature map. 
        Returns: The feature map downscaled with feature extraction.
        """
        # Apply feature extractions
        x = self.lrelu(self.conv1(x))

        # Downscale the feature map
        x = self.downscale(x)

        return x


def fully_connected(in_channels, out_channels):
    """
    Represents one fully connected layer of a standard neural network with leaky
    ReLu activations.

    Param: in_channels: The size of the input vector to this layer
    Param: out_channels: The size of the output vector this layer creates
    Return: A sequential model of one fully connected layer 
    REF: This function was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    """
    return nn.Sequential(
        nn.Linear(in_channels, out_channels),
        nn.LeakyReLU(hp.LRELU_SLOPE_ANGLE)
    )


class PixelNorm(nn.Module):
    """
    Used to perform pixel normalisation on a vectorised image or latent space vector. The vector
    is normalised relative to it's own mean and variance.

    REF: This class was inspired by the following website:
    REF: https://blog.paperspace.com/implementation-stylegan-from-scratch/
    """

    def __init__(self):
        """
        An instance of the PixelNorm class.
        """
        super(PixelNorm, self).__init__()
        self.epsilon = hp.EPSILON # Used to avoid divison by zero

    def forward(self, x):
        """
        Normalises the input vector x, reltaive to it's own mean and variance. 

        Param: x: The input vector to be normalised.
        Return: A normalised output vector.
        """
        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + self.epsilon) 


class AdaIN(nn.Module):
    """
    Within a StyleGAN model, the Adaptive Instance Normalisation (AdaIN) is used
    to adjust the mean and variance of feature maps based on the style vector w. This
    injection of the style vector's information allows the model to control the output of
    the image's style.

    REF: This class was inspired by code generated by ChatGPT-4o via the following prompt:
    REF: Prompt: Can you show me code for a StyleGan model and break down each section so that I can understand it?
    REF: It was also inspired by the following website: 
    REF: https://blog.paperspace.com/implementation-stylegan-from-scratch/ 
    """

    def __init__(self, channels):
        """
        An instance of the AdaIN class. The instance normalisation is initialised based on the 
        channels of the input tensor x, and the scale and shift vectors are created as a 
        fully connected layer each.

        Param: channels: The amount of channels of the input tensor x.
        """
        super(AdaIN, self).__init__()
        self.instance_norm = nn.InstanceNorm2d(num_features=channels, eps=hp.EPSILON)
        self.style_scale = fully_connected(hp.LATENT_SIZE, channels)
        self.style_shift = fully_connected(hp.LATENT_SIZE, channels)

    def forward(self, x, w):
        """
        Performs AdaIN on the input tensor x, based on the input style vector w.

        Param: x: An input tensor representing a layer of information within the generator. 
        Param: w: The style vector that is used to scale and shift the x tensor.
        Return: The AdaIN of the x and w tensors.
        """
        x = self.instance_norm(x)
        scale = self.style_scale(w).unsqueeze(2).unsqueeze(3)
        shift = self.style_shift(w).unsqueeze(2).unsqueeze(3)

        return scale * x + shift
    

class Noise(nn.Module):
    """
    Within a StyleGAN model, the generator receives multiple injections of noise into each of
    it's layers to improve the output image's finer details and quality. This class facilitates 
    the noise injections, and even creates a learnable noise parameter to improve training further.

    REF: This class was inspired by the following website:
    REF: https://blog.paperspace.com/implementation-stylegan-from-scratch/
    """

    def __init__(self, channels):
        """
        An instance of the Noise class. A learnable noise parameter is created so that training can be
        enhanced by learning an appropriate noise scale. 

        Param: channels: The amount of channels within the input tensor x.
        """
        super(Noise, self).__init__()
        self.learned_noise = nn.Parameter(torch.zeros(1, channels, 1, 1))


    def forward(self, x):
        """
        Generates random noise and scales it by a learned factor, then applies it to the input tensor x.

        Param: x: An input tensor x, representing the information within a layer of the generator. 
        Return: The input tensor x with additional noise added.
        """
        noise = torch.randn((x.shape[0], 1, x.shape[2], x.shape[3]), device=x.device)
        return x + self.learned_noise * noise